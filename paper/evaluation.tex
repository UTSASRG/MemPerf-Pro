\section{Experimental Evaluation}
\label{sec:evaluation}

The experimental evaluation will answer the following questions:
\begin{itemize}
\item How is the effectiveness of \MP{}? (Section~\ref{sec:effectiveness}) 	
\item What is the performance overhead of \MP{}? (Section~\ref{sec:perf})
\item What is the memory overhead of \MP{}? (Section~\ref{sec:memory})
\item What are the range of metrics for performant allocators? (Section~\ref{})
\end{itemize}

Experiments were performed on a two-processor machine, where each processor is Intel(R) Xeon(R) Gold 6230. Each processor has 20 cores in total. This machine has 256GB of main memory, 20MB of L2 cache, and 1280KB L1 cache. The underlying OS is Ubuntu 18.04.3 LTS, installed with the Linux-5.3.0-40. All applications were compiled using GCC-7.5.0, with -O2 and -g flags.

\subsection{Effectiveness}
\label{sec:effectiveness}

In order to evaluate the effectiveness, we evaluate \MP{} with five general purpose allocators, e.g. two versions of the Linux allocator (version 2.21 and 2.28), TCMalloc~\citep{tcmalloc}, jemalloc, and Hoard, and two secure allocators, i.e. DieHarder and OpenBSD. These allocators include both sequential and BiBOP-style allocators. Secure allocators were included, since they have their memory management policies. 

For the evaluation, we use the default configurations of these allocators. However, we make some changes in order to the interception of synchronizations. Since Linux allocators are included in \texttt{glibc} libraries, they invokes the internal synchronizations as \texttt{lll\_lock}, which cannot be intercepted by \MP{}. They are compiled separately as a stand-alone library. Since Hoard are using \texttt{std::lock\_guard} for its synchronization, which cannot be intercepted, we replaced them with POSIX spin locks to track its synchronization behavior.

%\todo{Let's use a table to list all dramatic difference between these allocators. This gives us some evidence of allocators}
%\subsection{Issues Identified in Different Allocators}

\input{applications} 
\input{metrics}

%For a performant allocator, what's the common things within the average allocator. We could utilize a table to list the average points of each allocator. Potentially, we could utilize these parameters to evaluate a new allocator. 

%For evaluating purpose, we could provide two information, one is the average with all evaluated allocators, another one is to omit one allocator with the lowest scores. 


%It seems that BIBOP style allocators are the trend of allocators, which not only has a better performance overhead on average, but also has better safety by separating the metadata from the actual heap. 



\subsection{Performance Overhead}
\label{sec:perf}

We will evaluate the performance overhead of the profiler itself. 

\subsection{Memory Overhead}
\label{sec:memory}

We will evaluate the performance overhead of the profiler itself. 

\begin{comment}

\subsection{Range of Allocator Metrics}
We will provide the metrics to evaluate the allocators, based on the averaged value. 
\todo{What types of metrics should we used? For instance, what type of policy should we used to exclude an allocator, and then get the value of the allocator. 20\%}
We will provide a table that can be utilized to evaluate all future allocators. 
	
\end{comment}

