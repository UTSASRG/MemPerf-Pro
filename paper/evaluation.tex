\section{Experimental Evaluation}
\label{sec:evaluation}

The experimental evaluation will answer the following questions:
\begin{itemize}
\item How is the effectiveness of \MP{}? (Section~\ref{sec:effectiveness}) 	
\item What is the performance overhead of \MP{}? (Section~\ref{sec:perf})
\item What is the memory overhead of \MP{}? (Section~\ref{sec:memory})
\item What are the range of metrics for performant allocators? (Section~\ref{})
\end{itemize}

Experiments were performed on a two-processor machine, where each processor is Intel(R) Xeon(R) Gold 6230. Each processor has 20 cores in total. This machine has 256GB of main memory, 20MB of L2 cache, and 1280KB L1 cache. The underlying OS is Ubuntu 18.04.3 LTS, installed with the Linux-5.3.0-40. All applications were compiled using GCC-7.5.0, with -O2 and -g flags.

\subsection{Effectiveness}
\label{sec:effectiveness}

In order to evaluate the effectiveness, we evaluate \MP{} with five general purpose allocators, e.g. two versions of the Linux allocator (version 2.21 and 2.28), TCMalloc~\citep{tcmalloc}, jemalloc, and Hoard, and two secure allocators, i.e. DieHarder and OpenBSD. These allocators include both sequential and BiBOP-style allocators. Secure allocators were included, since they have their memory management policy, representing some unique possibilities for \MP{}. 

For the evaluation, we use their default configuration. However, we do make some changes, mostly due to the interception of synchronizations. Since Linux allocators are included in \texttt{glibc} libraries, they invokes the internal synchronizations as \texttt{lll\_lock}, which cannot be intercepted by \MP{}. They are compiled separately as a stand-alone library. Since Hoard are using \texttt{std::lock\_guard} for its synchronization, which cannot be intercepted, we replaced them with POSIX spin locks to track its synchronization behavior.

\todo{Let's use a table to list all dramatic difference between these allocators. This gives us some evidence of allocators}



For a performant allocator, what's the common things within the average allocator. We could utilize a table to list the average points of each allocator. Potentially, we could utilize these parameters to evaluate a new allocator. 

For evaluating purpose, we could provide two information, one is the average with all evaluated allocators, another one is to omit one allocator with the lowest scores. 


It seems that BIBOP style allocators are the trend of allocators, which not only has a better performance overhead on average, but also has better safety by separating the metadata from the actual heap. 

\subsection{Issues Identified in Different Allocators}

\paragraph{glibc-2.21:}
This allocator of glibc-2.21 has a bug that invokes excessively large number of \texttt{madvise} systems calls under certain memory use patterns~\cite{madvise}, which is exhibited clearly when running the dedup application. \MP{} reports around $XX$ invocations of \texttt{madvise} per second, and the runtime of each \texttt{madvise} is about $xx$ cycles. That is clearly indicates that too many \texttt{madvise} system calls introduces kernel contention inside. 

\paragraph{DieHarder:}
DieHarder often performs much slower than other secure allocators for many applications, with some cases as high as $8x$ runtime overhead compared to the default Linux allocator. This can been attributed to several design elements of DieHarder, including its use of bitmaps to track heap objects, and the specific implementation details regarding how randomization is performed. Upon allocation, objects are chosen from a potentially large number of ``miniheaps'', without regard for the temporal locality of previously freed objects. Additionally, if the chosen object is currently in-use, the allocator will then continue to repeat this random selection process until it is success in locating an available object. These design decisions result in higher cache misses, as well as an increase in instruction count along allocation/deallocation pathways.
As evidence for this, we can see the results of \MP{}'s analysis of DieHarder when used to run the \texttt{dedup}, which runs about $3.4x$ slower as compared to glibc 2.24. \MP{} reports a $320x$ increase in CPU cycles for newly allocated objects in DieHarder, as well as a $3.7x$ increase in cycles for reused objects, and a $7.7x$ increase in cycles for freed objects.
While part of this increase is attributable to the over $16x$ increase in allocation instructions, there is also a large and noticeable difference between the number of mutex waits performed ($24x$), which corresponds to an increase of $1552x$ more cycles spent waiting on these mutex locks. While there were roughly the same number of critical sections entered by both allocators, DieHarder spent $26x$ longer within them than did glibc.
Finally, there was significantly more cache line owner conflicts seen in DieHarder than in glibc (a ratio of about $14x$). Application friendliness data shows that DieHarder had significantly lower values for average cache and page utilization, about 45\% and 57\% lower, respectively.

Next, in the case of freqmine, DieHarder performs $8.8x$ slower than glibc 2.24. \MP{} reveals that in this case, DieHarder had $58x$ more CPU cycles spent within in the object-reuse allocation pathway, and $61x$ more cycles spent in the object deallocation path.
	Additionally, we see $102x$ more read TLB misses along reused object allocation paths, and $188x$ more on deallocation paths.
	
	Further, we see $7091$ mmap system calls with DieHarder versus only 189 for glibc, as well as almost $10x$ mutex lock acquisitions, resulting in an enormous increase in the number of cycles spent waiting on these lock acquisitions (about 5 million times longer).
	Finally, we see DieHarder with about $4x$ as many cache owner conflicts as in glibc.

\paragraph{jemalloc:}
During evaluation, the \texttt{reverse\_index} benchmark was found to perform approximately 21\% slower when paired with \texttt{jemalloc} versus the default Linux allocator. Upon inspection, we find that, with \texttt{jemalloc}, the program exhibited over $2x$ the number of CPU cycles associated with the deallocation execution path, as well as a 34\% increase in critical section duration (i.e., the cycles spent within outermost critical sections).

\paragraph{TCMalloc:}
Despite its BIBOP-style memory layout, \texttt{tcmalloc} typically performs very well, often performing as well or better than the default Linux allocator. However, in the case of \texttt{swaptions}, it runs around 21\% runtime slower compared to the default Linux alocator. Using \MP{}, we find that the runtime of new allocations  is $2.6x$ greater than that of \texttt{glibc}. We additionally see (albeit smaller) increases in the number of reused object allocation cycles as well as deallocation cycles.



\subsection{Performance Overhead}
\label{sec:perf}

We will evaluate the performance overhead of the profiler itself. 

\subsection{Memory Overhead}
\label{sec:memory}

We will evaluate the performance overhead of the profiler itself. 

\subsection{Range of Allocator Metrics}
We will provide the metrics to evaluate the allocators, based on the averaged value. 
\todo{What types of metrics should we used? For instance, what type of policy should we used to exclude an allocator, and then get the value of the allocator. 20\%}
We will provide a table that can be utilized to evaluate all future allocators. 

