\section{Experimental Evaluation}
\label{sec:evaluation}

We will investigate the following allocators. We choose three general purpose allocators, e.g. jemalloc, TCMalloc, and Hoard; and two secure allocators, i.e. DieHarder and OpenBSD; in addition to the default allocator of the Linux system (in versions 2.21 and 2.24). Secure allocators will employ the randomization for memory allocations, which will increase runtime overhead due to decreased cache locality and increased instruction counts.
%For the default allocator, we actually have to 

\subsection{Performance Overhead}
We will evaluate the performance overhead of the profiler itself. 

\subsection{Memory Overhead}
We will evaluate the performance overhead of the profiler itself. 

\subsection{Comparison of Different Allocators}

\subsubsection{Performance Overhead}



\subsubsection{Memory Overhead}

\subsubsection{Scalability}



\subsubsection{Application Friendliness}

\subsubsection{Conclusions}

For a performant allocator, what's the common things within the average allocator. We could utilize a table to list the average points of each allocator. Potentially, we could utilize these parameters to evaluate a new allocator. 

For evaluating purpose, we could provide two information, one is the average with all evaluated allocators, another one is to omit one allocator with the lowest scores. 


It seems that BIBOP style allocators are the trend of allocators, which not only has a better performance overhead on average, but also has better safety by separating the metadata from the actual heap. 

\subsection{Issues Identified in Different Allocators}

\paragraph{glibc-2.21:}
This version of the default Linux allocator has a known bug which causes it to make excessively large number of \texttt{madvise} systems calls under certain memory use patterns. This is exhibited clearly under the dedup test of the PARSEC suite.
	Using a non-buggy version of glibc (2.24), the allocator typically makes around $280$ calls to \texttt{madvise}, whereas the affected version (2.21) makes about $505,233$.
	This phenomenon is seen clearly in the \texttt{num\_advise} and \texttt{madvise\_wait\_cycles} output fields in the log produced by \MP{}.

%\paragraph{glibc-2.24:}

\paragraph{DieHarder:}
DieHarder often performs much slower than other secure allocators, with some cases as high as $8x$ runtime overhead compared to the default Linux allocator. This can been attributed to several design elements of DieHarder, including its use of bitmaps to track heap objects, and the specific implementation details regarding how randomization is performed. Upon allocation, objects are chosen from a potentially large number of ``miniheaps'', without regard for the temporal locality of previously freed objects. Additionally, if the chosen object is currently in-use, the allocator will then continue to repeat this random selection process until it is success in locating an available object. These design decisions result in higher cache misses, as well as an increase in instruction count along allocation/deallocation pathways.
As evidence for this, we can see the results of \MP{}'s analysis of DieHarder when used to run the \texttt{dedup}, which runs about $3.4x$ slower as compared to glibc 2.24. \MP{} reports a $320x$ increase in CPU cycles for newly allocated objects in DieHarder, as well as a $3.7x$ increase in cycles for reused objects, and a $7.7x$ increase in cycles for freed objects.
While part of this increase is attributable to the over $16x$ increase in allocation instructions, there is also a large and noticeable difference between the number of mutex waits performed ($24x$), which corresponds to an increase of $1552x$ more cycles spent waiting on these mutex locks. While there were roughly the same number of critical sections entered by both allocators, DieHarder spent $26x$ longer within them than did glibc.
Finally, there was significantly more cache line owner conflicts seen in DieHarder than in glibc (a ratio of about $14x$). Application friendliness data shows that DieHarder had significantly lower values for average cache and page utilization, about 45\% and 57\% lower, respectively.

Next, in the case of freqmine, DieHarder performs $8.8x$ slower than glibc 2.24. \MP{} reveals that in this case, DieHarder had $58x$ more CPU cycles spent within in the object-reuse allocation pathway, and $61x$ more cycles spent in the object deallocation path.
	Additionally, we see $102x$ more read TLB misses along reused object allocation paths, and $188x$ more on deallocation paths.
	
	Further, we see $7091$ mmap system calls with DieHarder versus only 189 for glibc, as well as almost $10x$ mutex lock acquisitions, resulting in an enormous increase in the number of cycles spent waiting on these lock acquisitions (about 5 million times longer).
	Finally, we see DieHarder with about $4x$ as many cache owner conflicts as in glibc.

\paragraph{jemalloc:}
During evaluation, the \texttt{reverse\_index} benchmark was found to perform approximately 21\% slower when paired with \texttt{jemalloc} versus the default Linux allocator. Upon inspection, we find that, with \texttt{jemalloc}, the program exhibited over $2x$ the number of CPU cycles associated with the deallocation execution path, as well as a 34\% increase in critical section duration (i.e., the cycles spent within outermost critical sections).

\paragraph{TCMalloc:}
Despite its BIBOP-style memory layout, \texttt{tcmalloc} typically performs very well, often performing as well or better than the default Linux allocator. However, in the case of \texttt{swaptions}, it produces a 21\% runtime overhead compared to glibc. Using \MP{}, we find that, in this instance the number of new allocation cycles, is $2.6x$ greater than that of glibc. We additionally see (albeit smaller) increases in the number of reused object allocation cycles as well as deallocation cycles.

