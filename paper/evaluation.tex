\section{Experimental Evaluation}
\label{sec:evaluation}

The experimental evaluation will answer the following questions:
\begin{itemize}
\item How is the effectiveness of \MP{}? (Section~\ref{sec:effectiveness}) 	
\item What is the performance overhead of \MP{}? (Section~\ref{sec:perf})
\item What is the memory overhead of \MP{}? (Section~\ref{sec:memory})
\end{itemize}

Experiments were performed on a two-processor machine, where each processor is Intel(R) Xeon(R) Gold 6230. Each processor has 20 cores in total. This machine has 256GB of main memory, 20MB of L2 cache, and 1280KB L1 cache. The underlying OS is Ubuntu 18.04.3 LTS, installed with the Linux-5.3.0-40. All applications were compiled using GCC-7.5.0, with -O2 and -g flags.

\subsection{Effectiveness}
\label{sec:effectiveness}

In order to evaluate the effectiveness, we evaluate \MP{} with five general purpose allocators, e.g. two versions of the Linux allocator (version 2.21 and 2.28), TCMalloc~\citep{tcmalloc}, jemalloc, and Hoard, and two secure allocators, i.e. DieHarder and OpenBSD. These allocators include both sequential and BiBOP-style allocators. Secure allocators were included, since they have their unique memory management policies. 

For the evaluation, we use the default configurations of these allocators. However, we make some changes in order to the interception of synchronizations. Since Linux allocators are included in \texttt{glibc} libraries, they invokes the internal synchronizations as \texttt{lll\_lock}, which cannot be intercepted by \MP{}. They are compiled separately as a stand-alone library. Since Hoard are using \texttt{std::lock\_guard} for its synchronization, which cannot be intercepted, we replaced them with POSIX spin locks to track its synchronization behavior.

%\todo{Let's use a table to list all dramatic difference between these allocators. This gives us some evidence of allocators}
%\subsection{Issues Identified in Different Allocators}

\input{applications} 
\input{metrics}

%For a performant allocator, what's the common things within the average allocator. We could utilize a table to list the average points of each allocator. Potentially, we could utilize these parameters to evaluate a new allocator. 

%For evaluating purpose, we could provide two information, one is the average with all evaluated allocators, another one is to omit one allocator with the lowest scores. 


%It seems that BIBOP style allocators are the trend of allocators, which not only has a better performance overhead on average, but also has better safety by separating the metadata from the actual heap. 



\subsection{Performance Overhead}
\label{sec:perf}

\begin{figure}[!ht]
\centering
\includegraphics[width=5.5in]{figures/perfoverhead}
\caption{Performance overhead of \texttt{mmprof}, normalized to the runtime of default Linux allocator.\label{fig:overhead}}
\end{figure}

We also evaluate the performance overhead of 
\MP{} using PARSEC~\citep{parsec},  Phoenix~\citep{phoenix}, and multiple synthetic applications from Hoard~\cite{Hoard}. The performance overhead can be seen in Figure~\ref{fig:overhead}, which is the average value of five executions. From this figure, we can see that \MP{} runs $2.6\times$ slower than the default allocator, where two applications (\texttt{histogram} and \texttt{threadtest}) impose over $5\times$ performance overhead. Based on our understanding, both the number of memory operations and the number of lock acquisitions can significant impact  the performance overhead of \MM{}. To help the explanation, we further collect the characteristics of these applications, as shown in Table~\ref{table:characteristics}.  

First, if an application invokes extensive applications and deallocations in a short period of time, then \MP{} may introduce a large performance overhead. For each memory operation, \MP{} invokes two RDTSC instructions to collect the runtime,  updates multiple counters, and updates the state in the global hash table.  Second, the number of lock acquisitions inside memory operations could also significantly affect the performance. Similarly, \MP{} also collects the runtime data of each lock acquisition via the RDTSC instruction, and update different counters. 
Since \texttt{canneal} invokes 11 million memory operations and 0.3 million locks acquisitions per second, and \texttt{threadtest} will has 6 million memory operations and 33 lock acquisitions per second, that explains why these applications have larger performance overhead with \MP{}.  

\texttt{histogram} program is an exception, since it has a small number of allocations. For this application, \MP{} has an initialization phase to perform the initialization, and an finalization phase to analyze the data and write out results to the external file. But \texttt{histogram} only takes 0.12 seconds to finish. Therefore, \MP{} adds more overhead than the program's execution time. \texttt{linear\_regression} has the same issue, with the total execution time of 0.3 seconds.  
%for some tiny applications like \texttt{histogram}, which only takes 0.1 second when running along, \MP{}'s initialization and conclusion would take more time than themselves. Thus, performance overhead ratios for tiny applications could be larger.
%\texttt{threadtest} actually imposes more performance overhead, due to the reason that most memory opeations are actually 



%some applications invoke allocations intensively and almost simultaneously from different threads, \MP{} may introduce more contention in the hash table when checking every object's status, and higher contention rates would cause programs' slowdown.
\begin{comment}
For example, according to \ref{sec:memory}, 
canneal 2.86x   11438194.73 (alloc+free)/sec    42282925 alloc+free 311044.69 lockacqs/sec
reverse_index 4.59x 973245.43 (alloc+free)/sec 40000387 alloc+free 120407.33 lockacqs/sec
threadtest 7.21x 6 228 715.40 (alloc+free)/sec 256000203 alloc+free 33 239 532.98 lockacqs/sec



For example, according to \ref{sec:memory}, 
linear_regression 4.56x, 0.3s when running alone, 2 alloc+free
word_count 4.26x, 1.71s when running alone, 1 alloc
histogram 10.23x, 0.12s when running alone, 4 alloc+free

Upon every allocation and deallocation, \MP{} collects the runtime and acquisition information.

\end{comment}

\begin{table}[h]
  \centering
  \footnotesize
  \setlength{\tabcolsep}{0.2em}
\begin{tabular}{l|c|r|r|r|r}
\hline
\multicolumn{1}{c|}{Application} & 
\multicolumn{1}{c|}{Runtime}    & 
\multicolumn{1}{c|}{New Alloc}     & 
\multicolumn{1}{c|}{Reused Alloc}     & 
\multicolumn{1} {c|}{Free}     & 
\multicolumn{1}{c}{Lock Acqs} \\ \hline
  blackscholes & 16.7 & 8 & 1 & 7 & 11 \\ \hline   
   bodytrack & 8.5 & 20150 & 460616 & 480765 & 871397 \\ \hline    
   cache-scratch & 3.0 & 44 & 400000 & 400043 & 47 \\ \hline    
   cache-thrash  & 2.4 & 43 & 3999960 & 4000002 & 45\\ \hline  
   canneal & 29.4 & 8756242 & 12385221 & 21141462 & 9144714 \\ \hline    
   dedup & 12.7 & 3384984 & 683368 & 1750378 & 4864027 \\ \hline    
   facesim & 159.2 & 953143 & 3955049 & 4094483 & 1678963 \\ \hline    
   ferret & 25.3 & 149680 & 236867 & 415914 & 417370\\ \hline    
   fluidanimate & 12.3 & 229912 & 1 & 229913 & 307124 \\ \hline    
   freqmine & 20.2 & 1810 & 4 & 1070 & 15926 \\ \hline    
   histogram & 0.12 & 2 & 0 & 2 & 3 \\ \hline    
   kmeans & 16.4 & 200691 & 533 & 200579 & 303705 \\ \hline    
   larson & 15.1 & 2408955 & 33726797 & 36095750 & 38088835 \\ \hline   
   linear_regression & 0.3 & 1 & 0 & 1 & 2 \\ \hline    
   matrix_multiply & 4.8 & 83 & 0 & 82 & 85 \\ \hline    
   pca & 9.2 & 16131 & 29 & 72 & 16466 \\ \hline    
   raytrace & 41.1 & 5000115 & 15000100 & 20000172 & 5000240 \\ \hline   
   reverse_index & 1.5 & 1632810 & 106173 & 1738982 & 1806110\\ \hline  
   streamcluster & 23.5 & 47 & 8798 & 8844 & 17622\\ \hline    
   string_match & 0.6 & 8 & 0 & 7 & 10 \\ \hline    
   swaptions & 14.5 & 2040 & 47999756 & 48000385 & 48002039\\ \hline    
   threadtest & 7.7 & 1280122 & 126720000 & 128000081 & 255944404\\ \hline    
   vips 6.5 & 8128 & 1420072 & 1428019 & 1526404\\ \hline    
   word_count & 1.7 & 1 & 0 & 0 & 2\\ \hline   
   x264 & 24.2 & 10 & 0 & 9 & 13\\ \hline    \hline 
   
     \hline
  \end{tabular}
  \caption{Characteristics of applications\label{table:characteristics}}
\end{table}
\subsection{Memory Overhead}
\label{sec:memory}
\input{memory}


\begin{comment}

\subsection{Range of Allocator Metrics}
We will provide the metrics to evaluate the allocators, based on the averaged value. 
\todo{What types of metrics should we used? For instance, what type of policy should we used to exclude an allocator, and then get the value of the allocator. 20\%}
We will provide a table that can be utilized to evaluate all future allocators. 


%Jin


\end{comment}

