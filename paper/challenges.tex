Basically, \MP{} utilizes the hardware Performance Monitor Units (PMU), RDTSC timestamp, and simple counters together to perform the profiling. For instance, it utilizes the RDTSC timestamps to collect the execution time of each allocation/deallocation, and utilize simple counters to obtain some statistics information, e.g. the number of allocations and deallocations, and so on. It also employs PMUs to collect  hardware events of each allocation and deallocation, such as TLB read/write misses, retired instructions, and page faults. Different from existing work, \MP{} proposes \textbf{the novel usage of PMUs to evaluate application friendliness}, such as cache utilization (and contention) ratio and page utilization ratio, which may indirectly affect cache misses and TLB misses. \MP{} samples memory accesses, and then obtains the cache utilization ratio and page utilization ratio in which each sampled access is located, and utilizes the sampled ratio to represent the overall ratio. Therefore, \MP{} could quantify the potential performance impact of a specific allocator on applications.    

However, there exists multiple implementation challenges. The most important challenge is the \textbf{overhead challenge}, where one careless design may impose up to 100 $\times$ overhead, based on our experience of the development. The huge overhead could be unaffordable even for development phases. More importantly, the significant overhead may skew the evaluation results unnecessarily. \MP{} takes multiple approaches to reduce the performance overhead. First, \MP{} designs a fast lookup mechanism that enables the fast checking on the size information of each object, and on the cache line usage and page usage upon each sampled access quickly, as further discussed in Section~\ref{sec:fastlookup}. This lookup mechanism is very difficult to design, since different allocators may have different behaviors, and their memory mappings are out of the control of \MP{} (without some obvious rules). Second, \MP{} minimizes the cache contention by utilizing thread-local recording, and only summarizing the data together in the end of execution. Third, \MP{} also reduces the allocation overhead of its recording data structures, by preallocating the space for internal memory usage. For instance, it employs the vast address space of 64-bits machines to design its lookup mechanism, and pre-allocates a huge space for saving the mapping. 

Another significant challenge comes from the adaption to different allocators. Specific issues include the following ones: (1) How to obtain the specific details of different allocators, such as size class information, type of allocator, metadata size information? (Section~\ref{}) (2) How to design the fast but general lookup mechanism for different allocators (Section~\ref{})? There are multiple factors may affect the design: the default Linux allocator extends its heap differently with the \texttt{sbrk} system call, making one of its heap arena far from other arenas (making it difficult to design the virtual memory)? Secure allocators, such as OpenBSD and DieHarder, may invoke one \texttt{mmap} to obtain one page from the OS. That is, there are a large number of mappings inside the OS. This fact makes some general data structures not suitable to support the fast lookup, such as a hash map or the range tree. (3) How to measure the kernel contention inside the OS (Section~\ref{})? The Linux's glibc-2.21 has a very serious performance problem that is caused by the kernel contention, due to its careless invoking of system calls (\texttt{madvise}). (4) How to measure the user-space contention for the Glibc's allocator (Section~\ref{})? 

\begin{comment}
Challenge 2: how to perform the profiling? Similar to existing work, we majorly use the time (supported by RTDSC), the number (instrumentation-based counting), and some hardware events (PMU events) to perform the sampling. The sampling approach will be similar to existing work, but we attribute those events to the memory management events, such as allocations and deallocations. 

Challenge 3: how to reduce the performance overhead? In order to reduce the number of cache contention, we re-design our data structure to avoid false sharing and true sharing as much as possible. Also, we 

Challenge 4: we propose a novel method to evaluate the application friendliness. We evaluate the cache friendliness, or TLB friendliness. 

Challenge 5: we employs an internal allocator to avoid the interfering with allocations and deallocations of applications.  

 
\MP{} utilizes multiple methods to minimize the performance overhead of the profiling.  

\end{comment}

\MP{} also avoids the pollution on the profiling data. For instance, since \MP{} is designed to profile the allocators on specific applications, its internal memory allocations should be separated from those  of applications. \MP{} avoids the change of different allocators by intercepting allocations/deallocations and system calls through the preloading mechanism, which makes it a drop-in library. Overall, \MP{} only introduces around $2\times$ performance overhead and around $3\times$ memory overhead, while providing abundant information about allocators itself. During its experimentation, \MP{} is employed to discover various issues of existing popular allocators. It further presents the first quantitative comparisons on different allocators, not limiting to the external performance comparison on different applications.  

%\MP{} is also adapted to different allocators, which employs a  test program to obtain the details of different allocators, such as the type of the allocator, size class information, and the metadata overhead of each object. 


\begin{comment}

1. Maybe we should detect the contention rate. If the last write is from a different thread, we will detect one contention. 
 
allocator: can we use some different configurations of the same allocator?
Can we use the same allocator on different applications, achieving different allocators?  
}





performance overhead: 
1. Using the hash maps to identify the size of each object is very slow. 
2. Turning multiple reads into one read around 2 or three times. 
3. Using the new mapping mechanism. 

How we can do that for glibc. We migrate the glibc as separate library, allowing us to intercept system or libraries. 

How to figure out the metadata information?
	
\end{comment}

 

