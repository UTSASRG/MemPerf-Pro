\section{Overview of mmprof}

\MP{} is designed as a drop-in library that can be simply linked to applications (and allocators), without the requirement of changing the code or even re-compilation. Although \MP{} could employ the binary-instrumentation to perform a more detailed profiling, such as identifying the instructions within each allocation or deallocation, the binary instrumentation may impose prohibitive performance overhead. With a high overhead, the profiling results may be significantly skewed, such as the waiting time of each lock acquisition inside the allocation. Instead, \MP{} employs the PMU hardware, RDTSC timestamp hardware, and simple counters together to perform the profiling.  
 
As described above, \MP{} focuses on two aspects of the allocator, the allocator itself and its potential impact on applications (application friendliness). In order to adapt to different allocators, a small program is designed to collect the allocator's specific feature, such as the type of allocators, the size class information, or others. 

In the remainder of this section, we describe the profiled items for each category, and the basic idea of profiling. 

\subsection{Performance Overhead}

For performance overhead, \MP{} profiles the average data of each allocation and deallocation, instead of the summarized values over the whole execution. Per allocation/deallocation data helps identify internal issues inside, if the data is counterintuitive. For instance, \texttt{DieHarder} is identified to have a very high number (\todo{up to X}) of cache misses upon each deallocation, which clearly a deficiency. 

First, \MP{} collects the time spending on each allocation and deallocation. It intercepts allocations and deallocations of allocations, and then computes the RDTSC timestamp difference of before and after each operation as the execution time. The allocation and deallocation time is a very important metrics on the efficiency of an allocator.  

%TLB read misses/TLB write misses/page faults/cache misses/instructions. They are PMU-based. 
% 

Second, \MP{} further collects hardware PMU events for each allocation and deallocation, where the introduction of PMU is described in Section~\ref{sec:pmu}. The PMU events allow \MP{} discover the specific information of allocations/deallocations without the instrumentation, such as retired instructions, cache misses, and TLB read/write misses. 

%Based on our understanding, the hardware events, such as retired instructions, cache misses or TLB misses, will help reveal some implementation issues of an allocator. For instance, \MP{} detects that the DieHarder allocator has an excessive number of cache misses and TLB misses upon each deallocation, around 5 times of each deallocation, which is significantly larger than that of other allocators. By examining the code, we found out a serious implementation issue of the DieHarder allocator, which traverses all mini-heaps to identify the placement of each object. Obviously, this implementation is extremely slow, considering that every deallocation has to perform such expensive lookup. By fixing such issues, the DieHarder's performance improves around \todo{20\%}.     


\begin{comment}
Can we integrate the cache misses or page faults for each allocation and deallocation, so that we could identify the issue of DieHarder that invokes many unnecessary cache misses?

If we could correlate cache misses to each thread, then we could do this. 

If allocation and deallocation takes too much time, it could be caused by multiple reasons:

(1) First, it just takes a lot of instructions (could we find out the lapsed instructions for each thread?)
(2) It may be caused by not good algorithm? 
(3) It can be caused by lock contention?
(4) It can be caused by system call related contention?
\end{comment} 

\subsection{Memory Overhead}

Based on our understanding, the memory overhead of each allocator comes from three aspects: the metadata overhead, the alignment (or internal fragmentation), and the memory blowup. 
How much memory are due to memory re-utilization rate? For instance, we may not fully utilize the memory due to the randomization mechanism. We only care about physical memory waste. Then we should also investigate how much memory has been explicitly returned back to the OS. 

How much memory are due to the metadata itself? 

\subsection{Scalability Analysis} 

User space contention:
How many separate locks are explicitly utilized? 
How many lock acquisitions? How much time are spending on lock waiting for each thread, and in total?

How much time spending on kernel-space contention? For instance, we could infer from memory-related system calls, such as mmap, munmap, madvise, brk, or something else? 

That is, we may have to integrate with SyncPerf for doing this. We will borrow their implementation in order to do this. 

\subsection{Application Friendliness} 
How many page faults and cache misses that are caused by applications? 

How many remote accesses? How many interconnect messages? We may employ the PMU mechanism to identify the information.


\subsection{Background of Performance Monitoring Units}
\label{sec:pmu}

\subsection{Background of RDTSC}

\label{sec:rdtsc}

\subsection{Background of Allocators}

\label{sec:allcoator}

Bibop style

Bump-pointer style

Size classes.




