\section{Design and Implementation}
\label{sec:implementation}

This section discusses the implementation and design of \MP{} on profiling different aspects of an allocator as follows. 
%To profile an allocator, \MP{} intercepts memory allocations/deallocations, memory-related system calls, and synchronizations. This also indicates that an allocator should utilize standard APIs in order for \MP{} to collect corresponding information. However, the Linux allocator utilizes the internal implementation of synchronizations by default. For the profiling purpose, it should be changed to invoke explicit POSIX-APIs instead. Fortunately, most allocators do not need any change or the recompilation.  \MP{} profiles the performance, memory overhead, scalability, and application friendliness, as discussed in different subsections. 

 %It also discusses some common issues, such as adapting to different allocators, and the performance issue of collecting data. 

\subsection{Profiling Performance Data}

\label{sec:performanceimplement}

\MP{} profiles the average time of every memory management operation, such as allocations and deallocations, which will be utilized to predict the performance impact (as discussed in Section~\ref{sec:predict}). The RDTSC instruction is employed to collect the time due to its accuracy and low overhead. Basically, \MP{} intercepts all allocation invocations, such as \texttt{malloc}, \texttt{calloc}, and \texttt{realloc}, and memory deallocations, such as \texttt{free}, and collects the time before and after each operation. The difference between them is the runtime of this operation.  

% How to differentiate based the type? How to verify whether this a new/re-used allocation (efficiently)? 
Since an allocator typically has different execution paths for different types of objects, such as new or re-used allocations of small objects, or allocation of big objects, \MP{} further collects the information for each type separately. Note that some allocators, such as Hoard, even have a different execution path for  objects with medium sizes. Based on our evaluation, the average allocation time for small and big objects can be as large as $2256\times$ difference for the \texttt{glibc-2.21} allocator. The fine-grained data helps identify design issues for a specific type. \MP{} differentiates the types of allocations and deallocations as follows. To differentiate small and big objects, \MP{} checks the requested size with the threshold stored in the configuration file (as described in Section~\ref{sec:understandingallocators}). In order to differentiate new allocations from re-used allocations that typically have over $3\times$ performance difference, \MP{} utilizes a global hash table to track objects: every allocated object will be inserted into the hash table in its allocation time, and will be kept in the table; If an allocation is found to be in the hash table, then this allocation is a re-used allocation. Otherwise, it is a new allocation. 

\MP{} also differentiates operations for the serial and parallel phase. In the serial phase, there is just one thread, typically just the main thread. There are multiple threads in parallel phase, as its name implies. The operation of the parallel phase may require more than $3\times$ of time than that in serial phase, mainly caused by user-space or kernel space contention in the parallel phase. For the preciseness, \MP{} excludes the time for the first allocation, since many allocators will perform its initialization upon the first allocation.  

% How to use sampling mechanism to further reduce the overhead? 
\MP{} also employs the PMUs to collect hardware events of each memory operation, such as cache misses, page faults, TLB misses, and instructions. These hardware events are  significant supplements for identifying an issue, given that \MP{} is a non-intrusive profiler that cannot know the implementation details of an allocator. For instance, DieHarder is generally very slow, which often has an unusual number of cache misses for each deallocation. With the events of cache misses, we could identify a design issue of DieHarder: it traverses all memory bags one by one to determine the original bag for an allocation, causing an excessive number of cache misses. Similarly, \MP{} collects hardware events before and after each operation, and uses the difference of two counters as the number of events occurring inside an operation. The Linux kernel has supported PMUs starting from 2009 (Linux-2.6.31)~\cite{pmulinuxsupport}, where users could set up performance monitoring via the \texttt{perf\_event\_open} system call. However, the collection of hardware events is still the most expensive operation in \MP{}, which will incur more than $2\times$ slowdown on average if all memory operations will collect such events. Therefore, \MP{} samples hardware events for 1 out of 100 operations, which helps reduce the overhead to \todo{around $57\%$ on average}.  


\subsection{Profiling Memory Overhead}
\label{sec:profilingmemory}

For memory overhead, \MP{} reports the amount and the ratio of different types of memory overhead, such as internal fragmentation, memory blowup, and external fragmentation (and others), and real memory usage, so that programmers can pinpoint the issues of excessive memory consumption. \MP{} tracks real memory usage on pages precisely, since the OS will allocate a physical page only when any word of this page is assessed. \MP{} intercepts memory-related system calls in order to obtain the total memory usage. For an allocator, the difference between total and real memory usage is memory overhead, which is the sum of internal fragmentation, memory blowup, and external memory fragmentation. Note that the metadata will be attributed to the type of external fragmentation and others, which cannot be easily separated. It is challenging to measure these memory overhead precisely and efficiently, as discussed in the following. 

For a small object, internal fragmentation is the difference between the size class and the requested size. But we cannot compute in this way, if the difference is larger than a page. We need to consider page allocation policy that the OS always allocates a physical page in a demanding way, even if only one word is used. For such objects, internal fragmentation should be computed differently: for a new allocation, the internal fragmentation will be difference between the end of the object and the end of its last page; For a re-used object, \MP{} should track the maximum pages of this object, and use the difference between the end of its last page and the end of the object as internal fragmentation. When an object is released, its internal fragmentation should be decremented. \MP{} utilizes the same hash table as described in Section~\ref{sec:performanceimplement} to track the maximum pages for each object. 

It is also challenging to compute memory blowup. By definition, a new allocation will be treated as memory blowup if there exist freed objects of this size class in the freelists of other threads. However, this definition does not specify whether memory blowup should be reduced upon the succeeding deallocation and re-allocation. Instead, \MP{} is based on such an observation: \textit{all freed objects of a size class represent the upper bound for its memory blowup; This upper bound subtracted by the size of recently-freed objects will be memory blowup for a size class}. Based on this observation, \MP{} further implements a practical method that uses a global counter to track recently-freed objects for each size class. This global counter is incremented upon every deallocation, but will be decremented upon each allocation if the counter is greater than zero. \MP{} will compute memory blowup for each size class of small objects and for big objects, and then utilizes the summary as the final memory blowup. After getting the memory blowup, \MP{} will compute external memory consumption (and others) that is equal to the subtraction of memory blowup and internal fragmentation from total memory overhead.


In the end, \MP{} will report memory wastes when memory consumption of an application reaches its maximum value. However, it is very expensive to frequently collect the data of memory wastes. Instead, \MP{} only updates this data, when the total memory usage is increased by 1MB. Also, it is also very expensive to update all data frequently using global counters, since this will cause significant cache contention. Therefore, \MP{} tracks most data with thread-local variables, and summarizes all data together when necessary. \MP{} only uses global variables for total memory consumption and counters of recently-freed objects for each size class. 
%However, even with global variables, it is still expensive to update them with the full synchronization or even atomic variables. Instead, \MP{} updates these variables without synchronization, which may lose some updates. However, based on our evaluation, \MP{} should still provide a reliable percentage for different types of memory overhead.  

\subsection{Profiling Scalability}
\label{sec:profilingscale}

\MP{} reports the scalability of a memory allocator. More specifically, \MP{} evaluates the scalability for both user space and kernel space. 

For user-space scalability, \MP{} mainly collects the number of lock acquisitions, the runtime of each lock acquisition and each critical section, and the number of contentions for each lock and in total. \MP{} utilizes the RDTSC instruction to collect the runtime. The challenging part is to track the contention information, without re-implementing synchronizations. 
   
% How to determine the contention. 

For the kernel-space scalability, \MP{} focuses on memory-related system calls inside the allocation and deallocation paths, including \texttt{mmap}, \texttt{munmap}, \texttt{mremap}, \texttt{sbrk}, \texttt{madvise}, and \texttt{mprotect}. \MP{} profiles the runtime of each invocation with the RDSTC instruction, as well as the number of invocations made for each system call. In order to identify the issues of a specific execution path, \MP{} also collects the data for each type of allocation and deallocation, similar to the performance overhead discussed in Section~\ref{sec:performanceimplement}.
In fact, even simple data could actually uncover serious scalability issues of a memory allocator. For instance, the Linux allocator of \texttt{glibc-2.21} slows down an application by 20\%, which can be uncovered by the excessive number of \texttt{madvise} system calls and a higher runtime for the corresponding \texttt{mmap} and \texttt{mprotect} system call. 


For scalability data, we will also classify based on the allocation type. For those locks, we will provide per-lock data. 

\subsection{Application Friendliness}
\label{sec:profilefriendliness}

%Currently,

\begin{comment}
Cache line utilization: 
Shadow memory. 
Cache line: real using memory.
page: real used memory.

False sharing: 
Lines:
Cache contention rate: owner, if the current write operation is not the existing owner, we will increment the counter. 
We will report the percentage with write. 

False sharing, 
active/passive: 
how many lines with active and passive. 
	
\end{comment}

 
For application friendliness, \MP{} focuses on the following metrics, including cache/page utilization rate, cache contention rate, and false sharing effect. \MP{} employs PMU's to sample memory accesses periodically, with a default sampling period of $100,000$. Then, \MP{} updates these metrics upon every sampled access. \MP{} employs shadow memory to track this information in order to compute these values. 

For cache and page utilization rate, \MP{} maintains the number of used bytes for the  the current cache line and the current page, which will be updated on each allocation and deallocation. Upon every sampled event, \MP{} increments the number of cache lines and pages that have been accessed, and also increments the counter to track the total number of bytes on this cache line. In the end, \MP{} computes the utilization rate with a simple division. For cache utilization rate, the dividend is the total number of used bytes, and the divisor is the the total number of bytes for these cache lines. The page utilization rate is computed similarly, but focusing on the page level instead. 

However, the challenge is to quickly locate the metadata for each cache line and page, since the metadata should be updated upon every allocation and deallocation, as well as upon every sampled event. During its implementation, \MP{} evaluated multiple mechanisms. First, \MP{} designed a red-black tree to hold memory mappings of the heap, and then stored the address of corresponding metadata on the tree node. This mechanism was found to be inefficient, as some allocators (e.g., OpenBSD) include thousands of mappings, which may unfortunately introduce tens of comparisons. Second, \MP{} utilized a hash map to store the memory mappings. However, it is difficult to determine the optimal number of buckets for the hash table, where a small number may cause too many conflicts, with a significant performance overhead imposed by traversing the linked list. Finally, \MP{} designs a fast lookup mechanism by taking advantage of the vast address space of 64-bit machines, with the detailed design discussed as follows. 

% Based on our observation, all allocators invoke either \texttt{sbrk} or \texttt{mmap} system calls to obtain the memory from the underlying OS. The address range returned from \texttt{sbrk} is generally lower than 4G, while the range returned by \texttt{mmap} is typically less than 128TB. 
%Given that modern processors typically support 48 bits address space (256 TB), \MP{} employs the last TB (between 255TB and 256TB) of address space to store the meta data of object, with the design illustrated in Figure~\ref{fig:lookup}. 

\textbf{Three-Level Fast Lookup Mechanism:} \MP{} designs a three-level lookup mechanism as illustrated in Fig.~\ref{fig:lookup}, borrowing the idea from the multi-level page table design of operating systems. Basically, an ``MB Mapping'' will be the first level, where the index can be computed simply by dividing an address with 1 megabytes (MB). Each entry of this MB mapping points to all possible pages inside the current 1-megabyte memory. Since one megabyte of memory will have at most 256 pages -- given a 4KB size for each page -- each MB entry points to 256 page entries. Similarly, each page entry contains the information about the used bytes within this page and has a pointer pointing to 64 possible cache entries inside. Based on this design, it takes two steps to acquire the used bytes for a page, and three steps to obtain the used bytes for the current cache line. Therefore, it has the $\mathcal{O}(1)$ 
complexity to obtain the metadata.  
          
\begin{figure*}[!h]
\centering
\includegraphics[width=\columnwidth]{figures/lookup}
\caption{Three-level lookup mechanism of \MP{}.\label{fig:lookup}}
\end{figure*}

Note that this design is also efficient in memory consumption. If a range of addresses are not used, then there is no need to allocate physical memory for the corresponding page entries and cache entries. This design is able to adapt to different allocators, where memory mappings of a heap is varied from a few to hundreds of thousands, and these mappings can be scattered along the whole address space of a process. 
%To track valid memory mappings dynamically, \MP{} intercepts memory related system calls inside allocations/deallocations, such as \texttt{sbrk}, \texttt{mmap}, \texttt{munmap}, \texttt{mremap}. 

For false sharing effect, \MP{} focuses on two aspects: the number of cache lines that has false sharing, either active or passive false sharing, and the number of cache contention events on these cache lines. Identifying active and passive false sharing is relatively intuitive based on the definition, by checking the number of threads in each cache line. We employ three-level lookup table as discussed above to store the threads information that are allocated from the same cache line. 
For cache contention events, \MP{} relies on sampled memory accesses. Upon each sampled event, \MP{} checks whether the event is a write access. For each write, \MP{} updates the last thread to write on the corresponding cache line to be the current thread. If the current thread is different from the recorded last thread, \MP{} increments the number of cache contention events. In the end, \MP{} reports the percentage of cache contention on identified cache lines with false sharing issue. 

\MP{} also reports the number of cache contention events on cache lines without active/passive false sharing issues, using the same method. In fact, cache contention events on these cache lines can be caused by internal-object false sharing or true sharing, but they cannot be solved with a different memory allocator. However, the reported data could help understand whether an application is running slow or not. Note that \MP{} is not designed to be false/true sharing detection tool. If \MP{} reports a big cache contention rate, users may resort to specific tools to identify specific issues inside the application~\cite{Sheriff, Predator, DBLP:conf/ppopp/ChabbiWL18}. 
   

\begin{comment}


\subsection{Predicting Performance Impact}
\label{sec:predict}

In order to help users determine whether an allocator is the culprit of the performance issue, \MP{} further predicts the potential performance improvement after switching to  a better allocator. Predicting the performance impact is very complicated, since an application can be affected by multiple factors, such as hardware/software  contention and synchronization. From the point view of an allocator, it can be affected by both cache friendliness and memory management overhead. \MP{} only focuses on the latter one, which provides an lower bound on the potential improvement. Basically, \MP{} replaces the runtime of memory management operations with a standard runtime, and then predicts the reduction of the total runtime.  

For the standard runtime of every memory management operation, \MP{} utilizes the median values collected from a range of applications. Basically, based on our evaluation, the default Linux, TcMalloc, and jemalloc are three best allocators in terms of the performance. Then we choose the best allocator for each application in terms of cycles for each memory management operation, and then select the median value for each operation. Note that there are different choices to choose the standard runtime for every operation, and we only utilizes a simple one to proof the concept. We have thought about that the runtime of a memory management operation can be related with the parallelization and the frequency of allocation. However, it is challenging to build a direct relationship based on our evaluation results. For instance, a serial allocation can also take tens of thousands of cycles. 

To predict the runtime reduction, \MP{} further collects the runtime outside memory management operations with the RDTSC instruction. 
     
	
\end{comment}

\subsection{Prediction on Performance Impact}

\label{sec:predict}

\MP{} evaluates the performance impact that can be caused by the allocator. It utilizes the collected . 

What type of data will be predicted. We could not predict the potential performance impact caused by passive/active false sharing, or other cache-friendliness. 

But we could predict the performance impact by allocation's implementation. Basically, we will replace the time. 

% How to handle multithreaded applications? 

% How to deal with the baseline? 
Currently, we are using the data of TcMalloc as the base line. But how to choose the right baseline. We are using the averaged runtime for each type of cycles. 

% How to deal with the allocation type difference? For instance, one small allocation will be treated as big allocation in another allocators. 


\todo{Track the page faults and madvise}

\subsection{Collecting Allocator-Specific Information}
\label{sec:understandingallocators}

\MP{} is designed as a general profiler for sequential and BiBOP-style allocators, as described in Section~\ref{sec:allocator}. The primary challenge is to adapt to different allocators. \MP{} interprets a configuration file to identify the differences and unique characteristics of every allocator during its initialization phase, such as the allocator's style (BiBOP-style versus sequential), the sizes of its different classes, and the threshold separating small objects from large objects. Alternatively, this configuration can also be provided manually. \MP{} also provides a pre-run program to automatically determine these details for a given allocator.

In order to identify the style of allocator, the pre-run routine will check whether two subsequent allocations with different sizes (small objects, apparently from different size classes) are satisfied from the same virtual page. If they were, then the allocator is a sequential-style allocator, which is similar to the default Linux allocator. Otherwise, the allocator belongs to a BiBOP-style allocator. 

The second step is to identify the sizes of the various different size classes. The pre-run routine begins by allocating an object of 8 bytes, and continues to allocate additional objects using a stride increase of 8 bytes each time. The determination of size classes depends on the style of the allocator. For BiBOP-style allocators, an allocation with a different size class will be satisfied from a different bag, located in a different page. For sequential allocators, such as the Linux allocator, we employ the \texttt{malloc\_usable\_size} routine to return the bag size for an allocation of the specified size. 

%the distance between two contiguously-allocated objects (with distinct sizes) is utilized to determine the size class. As shown in Fig.~\ref{fig:sizeclass}, if the size of $Obj_1$ and $Obj_2$ is the same, judging from the distance of between two continuous objects, then they belong to the same size class. Otherwise, they belong to different size class, such as $Obj_3$ in the figure. By checking the size of two objects belonging to different objects, we could determine the sizes of different size classes.  
\begin{comment}

\begin{figure}[!ht]
\centering
\includegraphics[width=5in]{figures/sequentialclasssize}
\caption{Determining the size class of a sequential allocator by the distance between continuous allocations. \\The boxes with 10\% dotted pattern are the metadata, while the boxes with diagonal stripes\\ are actual heap objects. The number above each box represents the size of the corresponding object. \label{fig:sizeclass}}
\end{figure}
	
\end{comment}

The threshold for big objects are typically detected by checking whether there is an explicit \texttt{mmap} system call upon the allocation request. Typically, most allocators utilize a direct \texttt{mmap} system call to satisfy the allocation of a big object initially. However, this threshold requires manual confirmation.