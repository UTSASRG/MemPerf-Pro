In the following, we evaluate how \MP{} could benefit both normal users and allocator designers. 
\subsubsection{Benefiting Normal Users\\} 

\noindent \textbf{Prediction of Performance Impact:} 
\MP{} can predict the performance impact if switching to a new allocator as discussed in Section~\ref{sec:predict}. Here, we are utilizing the average cycles of TcMalloc listed in Table~\ref{tbl:metrics} to predict the performance impact of switching to TcMalloc. All applications in Figure~\ref{fig:motivation} are evaluated, except \texttt{cache-scratch},  \texttt{cache-thrash}, and \texttt{freqmine}.  Since \texttt{cache-scratch} and \texttt{cache-thrash} are running much slower with TcMalloc due to passive/active false sharing issue, their performance results with TcMalloc cannot serve as the baseline correctly. \texttt{freqmine} is an openmp program that \MP{} cannot support well. The prediction results can be seen in Table~\ref{tbl:predictionResult}, where ``reverse'' is the abbreviation of reverse\_index. 

\begin{table}[]
  \centering
  \footnotesize
  \setlength{\tabcolsep}{0.1em}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
 \multirow{2}{*}{APP} &
  \multicolumn{2}{c|}{Default} &
  \multicolumn{2}{c|}{glibc-2.21} &
  \multicolumn{2}{c|}{jemalloc} &
  \multicolumn{2}{c|}{TcMalloc} &
  \multicolumn{2}{c|}{Hoard} &
  \multicolumn{2}{c}{DieHarder} \\ \cline{2-13}
  & R & P & R & P & R & P & R & P & R & P  & R & P    \\ \hline
canneal        & 1.05 & 1.05 & 1.07 & 1.06 & 1.01 & 1.03 & 1.00 & 1.02 & 1.03 & 1.11 & 1.39 & 2.00 \\ \hline
dedup          & 1.06 & 1.01 & 1.35 & 1.01 & 1.06 & 1.00 & 1.00 & 1.00 & 1.02 & 1.00 & 2.91 & 1.89 \\ \hline
%freqmine       & 0.90 & 1.00 & 0.96 & 1.00 & 1.01 & 1.00 & 1.00 & 1.00 & 1.26 & 1.00 & 3.32 & 1.01 \\ \hline
kmeans         & 1.16 & 1.00 & 1.16 & 1.00 & 1.06 & 1.00 & 1.00 & 1.00 & 1.02 & 1.00 & 1.03 & 1.00 \\ \hline
raytrace       & 1.27 & 1.02 & 1.27 & 1.05 & 1.20 & 1.00 & 1.00 & 1.00 & 1.10 & 1.01 & 1.31 & 1.51 \\ \hline
reverse & 1.00 & 1.07 & 0.99 & 1.07 & 1.05 & 1.08 & 1.00 & 1.04 & 1.15 & 1.16 & 2.42 & 1.89 \\ \hline
swaptions      & 0.99 & 0.99 & 0.99 & 1.00 & 0.98 & 0.96 & 1.00 & 0.96 & 2.04 & 1.11 & 5.67 & 3.82 \\ \hline
\end{tabular}
   \caption{Normalized runtime to TcMalloc. ``R'' and ``P'' columns list real and predicted result.  \label{tbl:predictionResult}}
\end{table}

Overall, \MP{} could successfully predict serious performance impact (over 20\%) of  applications caused by an allocator (e.g., Hoard and DieHarder), although the exact numbers are different. For instance, \MP{} predicts that switching DieHarder to TcMalloc may boost the performance of \texttt{canneal} by $2\times$, but TcMalloc only improves the performance by 39\% in reality. Multiple reasons can contribute to the prediction difference or inaccuracy:

First, \MP{} is using the averaged cycles as the baseline for prediction. However, the  runtime in different applications can be affected by multiple factors, such as number of page faults, system calls, and lock contention. Some of them are very difficult to quantify, which are not considered during the prediction. 

Second, \MP{} could only predict the impact caused by slow memory management operations. However, the performance can be also affected by other factors, such as false sharing or cache misses. For instance, the runtime of memory management operations in \texttt{kmeans} is only a small portion of the total runtime (less than 3\%), with less than 2000 allocations. 

Third, \MP{} assumes no dependency between children threads of predicted applications, and predicts the performance based on the maximum parallel phase. However, in \texttt{dedup}, the longest thread was waiting for other threads, but it has not many allocations and deallocations. To be more precise, the prediction should consider the critical path~\cite{wPerf}, which is too complicated itself.

\textbf{Reporting Memory Overhead:} 
\MP{} shows high memory overhead caused by  allocators. 

According to our investigation, \texttt{canneal} has 72\% extra memory overhead when running with \texttt{DieHarder}, compared with the default Linux allocator. 
In \MP{}'s report, when \texttt{DieHarder} spends maximum memory on allocations, allocated objects only occupy 57\% of the total memory consumption. As when running with the default Linux allocator the percentage is 85\%, it shows that the extra memory consumption derives from \texttt{DieHarder}'s immature strategy of memory management. More precisely, \MP{} reports 10\% of \texttt{DieHarder}'s memory consumption is internal fragmentation of objects, 3\% is caused by memory blowup and the other 30\% comes from external fragmentation. 

One other example is \texttt{vips} when running with \texttt{jemalloc}, which spends 15\% more memory than the default Linux allocator. \MP{} presents \texttt{jemalloc} only effectively utilizes 59\% of its memory consumption, and percentages of internal fragmentation, memory blowup and external fragmentation are 2\%, 39\% and 0\%, which indicates \texttt{jemalloc} could have a memory blowup issue when running \texttt{vips}.

\textbf{Reporting Application Friendliness:} 
Due to the length limit, we will only present one example of page utilization, and another example of false sharing issues.

In \texttt{fluidanimate} running with \texttt{DieHarder}, \MP{} indicates its page utilization is only 79\%, while the default Linux allcator is 97\%. That abnormal number could indicate \texttt{DieHarder} does not fit well with the memory usage pattern and the access pattern of \texttt{fluidanimate}. Actually, \texttt{fluidanimate} that runs with \texttt{DieHarder} has 25\% higher memory overhead, and has 25\% more page faults than the default Linux allocator. All the aspects consistently show that \texttt{DieHarder} does not tap well with \texttt{fluidanimate}.

In \texttt{cache-scratch}, \texttt{TcMalloc} runs around $2.7\times$ slower compared to the default Linux allocator because of the passive false sharing issue.
\MP{} presents that when the allocator is \texttt{TcMalloc}, 22\% of the sampled instructions trigger a passive false sharing, while when using the default Linux allocator, only 3\% of instructions are detected with passive false sharing.
The result proves that \MP{} could detect whether a program is suffering from severe false sharing issues introduced by the allocator.

\subsubsection{Benefiting Allocator Designers}
In order to evaluate the effectiveness, we evaluate \MP{} with five widely-used allocators, including two versions of the Linux allocator (versions 2.21 and 2.28), TCMalloc~\cite{tcmalloc}, jemalloc, and Hoard, and two secure allocators, i.e. DieHarder and OpenBSD. These allocators include both sequential and BiBOP-style allocators. Secure allocators were included, since they have their unique memory management policies. 

For the evaluation, we use the default configurations of these allocators. However, we make some changes in order to  intercept synchronizations. Since the Linux allocators are included within the \texttt{glibc} libraries, they invoke the internal synchronizations (\texttt{lll\_lock}) directly, which cannot be intercepted by \MP{}. They are thus recompiled separately as a stand-alone library for the purposes of evaluation. Because Hoard is using \texttt{std::lock\_guard} for its synchronization, we replaced these with POSIX spinlocks to track its synchronization behavior.

\todo{Adding average cycles (range) for each type of allocations. This gives us some evidence of allocators (maybe includes the average number of locks??)}

\todo{What are workflow?}

\input{applications} 
\input{metrics}

%For a performant allocator, what's the common things within the average allocator. We could utilize a table to list the average points of each allocator. Potentially, we could utilize these parameters to evaluate a new allocator. 

%For evaluating purpose, we could provide two information, one is the average with all evaluated allocators, another one is to omit one allocator with the lowest scores. 


%It seems that BIBOP style allocators are the trend of allocators, which not only has a better performance overhead on average, but also has better safety by separating the metadata from the actual heap. 


