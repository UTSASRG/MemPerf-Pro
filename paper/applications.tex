\paragraph{glibc-2.21:}
This allocator of glibc-2.21 has a bug that invokes excessively large number of \texttt{madvise} systems calls under certain memory use patterns~\cite{madvise}, which is exhibited clearly when running the dedup application. \MP{} reports around $XX$ invocations of \texttt{madvise} per second, and the runtime of each \texttt{madvise} is about $xx$ cycles. That is clearly indicates that too many \texttt{madvise} system calls introduces kernel contention inside. \todo{Simply by changing the threshold of madvise, then the number of system calls is reduced by, and then the performance is improved by XX times.}

\paragraph{DieHarder:}
DieHarder often performs much slower than other secure allocators for many applications, with some cases as high as $8x$ runtime overhead compared to the default Linux allocator. This can been attributed to several design elements of DieHarder, including its use of bitmaps to track heap objects, and the specific implementation details regarding how randomization is performed. Upon allocation, objects are chosen from a potentially large number of ``miniheaps'', without regard for the temporal locality of previously freed objects. Additionally, if the chosen object is currently in-use, the allocator will then continue to repeat this random selection process until it is success in locating an available object. These design decisions result in higher cache misses, as well as an increase in instruction count along allocation/deallocation pathways.
As evidence for this, we can see the results of \MP{}'s analysis of DieHarder when used to run the \texttt{dedup}, which runs about $3.4x$ slower as compared to glibc 2.24. \MP{} reports a $320x$ increase in CPU cycles for newly allocated objects in DieHarder, as well as a $3.7x$ increase in cycles for reused objects, and a $7.7x$ increase in cycles for freed objects.
While part of this increase is attributable to the over $16x$ increase in allocation instructions, there is also a large and noticeable difference between the number of mutex waits performed ($24x$), which corresponds to an increase of $1552x$ more cycles spent waiting on these mutex locks. While there were roughly the same number of critical sections entered by both allocators, DieHarder spent $26x$ longer within them than did glibc.
Finally, there was significantly more cache line owner conflicts seen in DieHarder than in glibc (a ratio of about $14x$). Application friendliness data shows that DieHarder had significantly lower values for average cache and page utilization, about 45\% and 57\% lower, respectively.

Next, in the case of freqmine, DieHarder performs $8.8x$ slower than glibc 2.24. \MP{} reveals that in this case, DieHarder had $58x$ more CPU cycles spent within in the object-reuse allocation pathway, and $61x$ more cycles spent in the object deallocation path.
	Additionally, we see $102x$ more read TLB misses along reused object allocation paths, and $188x$ more on deallocation paths.
	
	Further, we see $7091$ mmap system calls with DieHarder versus only 189 for glibc, as well as almost $10x$ mutex lock acquisitions, resulting in an enormous increase in the number of cycles spent waiting on these lock acquisitions (about 5 million times longer).
	Finally, we see DieHarder with about $4x$ as many cache owner conflicts as in glibc.

\paragraph{jemalloc:}
During evaluation, the \texttt{reverse\_index} benchmark was found to perform approximately 21\% slower when paired with \texttt{jemalloc} versus the default Linux allocator. Upon inspection, we find that, with \texttt{jemalloc}, the program exhibited over $2x$ the number of CPU cycles associated with the deallocation execution path, as well as a 34\% increase in critical section duration (i.e., the cycles spent within outermost critical sections).

\paragraph{TcMalloc:}
Despite its BIBOP-style memory layout, \texttt{TcMalloc} typically performs very well, often performing as well or better than the default Linux allocator. However, in the case of \texttt{swaptions}, it runs around 21\% runtime slower compared to the default Linux alocator. Using \MP{}, we find that the runtime of new allocations  is $2.6x$ greater than that of \texttt{glibc}. We additionally see (albeit smaller) increases in the number of reused object allocation cycles as well as deallocation cycles.


\paragraph{Hoard:} 
We actually find out multiple un-known design issues of \texttt{Hoard}. By checking the output of \texttt{swaptions}, we have found the following issues: (1) the runtime of every allocation and deallocation is around XX times larger than the suggested metric. (2) the number of cache misses  is up to 6.6 for each re-used allocation, and 6.3XX for each deallocation. 